# ZetaOptimizer: A Riemann Zeta Functionâ€‘Inspired Optimizer for Deep Learning

## ğŸ§‘â€ğŸ’» Author

**Samiksha BC**
Indiana University


**ZetaOptimizer** is a novel stochasticâ€‘gradient optimizer that integrates the Riemann zeta function $\zeta(s)$ into the update rule.  It blends Adamâ€‘style momentum with dynamically scheduled zeta scaling, adaptive damping, and cosineâ€‘similarityâ€‘based boost.

> **Status**: Research prototype â€“ early empirical results on **MNIST**,  and **CIFARâ€‘100** show consistent gains over Adam and momentumâ€‘SGD.

---

## ğŸ”‘ Key Features

* **Zetaâ€‘scaled learning rate** â€“ update magnitude modulated by $\zeta(s)$ with a timeâ€‘varying exponent `s`.
* **Hybrid AdamÂ Ã—Â Zeta step** â€“ weighted mix (`adam_mix`) of Adam update and zetaâ€‘scaled gradient.
* **Adaptive damping** â€“ zeta damp factor driven by gradient norm and loss EMA.
* **Cosineâ€‘similarity momentum boost** â€“ encourages consistent gradient directions.
* **Gradient clipping & learningâ€‘rate cosine decay** for extra stability.

---

## ğŸ“¦ Installation

```bash
pip install torch torchvision scipy
```

PythonÂ â‰¥Â 3.9 recommended.  A CUDAâ€‘capable GPU is strongly encouraged; CPU also works but will be slow for CIFARâ€‘scale experiments.

---

## ğŸ“ Datasets & Notebooks

| Notebook                   | Dataset(s)      | Purpose                                        |
| -------------------------- | --------------- | ---------------------------------------------- |
| `Mnist_and_CIFARâ€‘10.ipynb` | MNIST, CIFARâ€‘10 | Quick reproduction of basic results            |
| `CIFARâ€‘100.ipynb`          | CIFARâ€‘100       | Largerâ€‘scale test with 100â€‘class image dataset |

Datasets are downloaded automatically via **torchvision**.

---

## ğŸ“Š Result Snapshot (5â€“20Â Epoch Runs)

| Optimizer         | MNIST Acc.  | CIFARâ€‘10 Acc. | CIFARâ€‘100 Acc. |
| ----------------- | ----------- | ------------- | -------------- |
| Adam              | 97.53â€¯%     | 50.72â€¯%       | 21.70â€¯%        |
| SGDÂ (+Â momentum)  | 97.69â€¯%     | 45.32â€¯%       | 22.51â€¯%        |
| **ZetaOptimizer** | **97.62â€¯%** | **52.20â€¯%**   | **25.71â€¯%**    |

These early runs use simple fullyâ€‘connected networks; deeper CNNs are expected to widen the performance gap.

---

## ğŸš€ Quick Start

```bash
python main.py  # trains on MNIST, CIFARâ€‘10, CIFARâ€‘100 sequentially and prints results
```

Tune hyperâ€‘parameters such as `lr`, `adam_mix`, `s_min/s_max`, or network architecture to explore further.

---


