{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKSRdF8jch1bNImUXvoHNM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameekshya1999/A-Riemann-Zeta-Function-Inspired-Optimizer-for-Deep-Learning/blob/main/Mnist_and_CIFAR_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l-9gnvsogkR",
        "outputId": "e7697cab-48c3-4d60-b22c-adffc6c0025b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0c17c6rGr-Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MNIST**"
      ],
      "metadata": {
        "id": "TFBbtTTmr3gk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sf0QNBR3r_Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "\n",
        "# Simple Feedforward NN for CIFAR-10 classification\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim=3*32*32, hidden_dim=128, output_dim=10):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Enhanced Zeta-inspired optimizer\n",
        "class ZetaOptimizer(optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, s_min=1.001, s_max=2.0, beta1=0.85, beta2=0.999, eps=1e-8, clip_norm=1.0, zeta_damp=0.3, adam_mix=0.5, total_steps=5000):\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(\"Learning rate must be positive\")\n",
        "        if s_min <= 1.0 or s_max <= 1.0:\n",
        "            raise ValueError(\"s_min and s_max must be > 1 for convergence\")\n",
        "        if not 0.0 <= beta1 < 1.0:\n",
        "            raise ValueError(\"Beta1 must be in [0, 1)\")\n",
        "        if not 0.0 <= beta2 < 1.0:\n",
        "            raise ValueError(\"Beta2 must be in [0, 1)\")\n",
        "        if eps <= 0.0:\n",
        "            raise ValueError(\"Epsilon must be positive\")\n",
        "        if clip_norm <= 0.0:\n",
        "            raise ValueError(\"Clip norm must be positive\")\n",
        "        if not 0.0 <= adam_mix <= 1.0:\n",
        "            raise ValueError(\"adam_mix must be in [0, 1]\")\n",
        "\n",
        "        defaults = dict(lr=lr, s_min=s_min, s_max=s_max, beta1=beta1, beta2=beta2, eps=eps, clip_norm=clip_norm, zeta_damp=zeta_damp, adam_mix=adam_mix, total_steps=total_steps)\n",
        "        super(ZetaOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        max_terms = 100\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            s_min = group['s_min']\n",
        "            s_max = group['s_max']\n",
        "            beta1 = group['beta1']\n",
        "            beta2 = group['beta2']\n",
        "            eps = group['eps']\n",
        "            clip_norm = group['clip_norm']\n",
        "            zeta_damp = group['zeta_damp']\n",
        "            adam_mix = group['adam_mix']\n",
        "            total_steps = group['total_steps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                grad = grad.clamp(min=-clip_norm, max=clip_norm)\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['m'] = torch.zeros_like(p.data)\n",
        "                    state['v'] = torch.zeros_like(p.data)\n",
        "                    state['grad_norm_ema'] = torch.zeros_like(p.data)\n",
        "                    state['prev_grad'] = torch.zeros_like(p.data)\n",
        "                    state['loss_ema'] = 1.0\n",
        "\n",
        "                state['step'] += 1\n",
        "                m, v = state['m'], state['v']\n",
        "                grad_norm_ema = state['grad_norm_ema']\n",
        "                prev_grad = state['prev_grad']\n",
        "                loss_ema = state['loss_ema']\n",
        "                step = state['step']\n",
        "\n",
        "                # Dynamic s scheduling\n",
        "                s = s_min + (s_max - s_min) * min(step / total_steps, 1.0)\n",
        "                zeta_s = sum(1.0 / (n ** s) for n in range(1, max_terms + 1))\n",
        "\n",
        "                # Per-parameter gradient norm\n",
        "                grad_norm_raw = torch.sqrt((grad ** 2).sum(dim=-1, keepdim=True))\n",
        "                grad_norm_ema.mul_(0.9).add_(grad_norm_raw, alpha=0.1)\n",
        "\n",
        "                # Adaptive damping based on loss and gradient norm\n",
        "                if closure is not None and loss is not None:\n",
        "                    state['loss_ema'] = 0.9 * loss_ema + 0.1 * loss.item()\n",
        "                adaptive_damp = zeta_damp * (1.0 + grad_norm_ema / (1.0 + grad_norm_ema)) * (1.0 / max(0.1, loss_ema))\n",
        "                zeta_factor = adaptive_damp / zeta_s * (1.0 / (1.0 + step * 0.005))\n",
        "\n",
        "                # Gradient consistency (cosine similarity)\n",
        "                grad_flat = grad.view(-1)\n",
        "                prev_grad_flat = prev_grad.view(-1)\n",
        "                cos_sim = torch.dot(grad_flat, prev_grad_flat) / (grad_flat.norm() * prev_grad_flat.norm() + eps)\n",
        "                momentum_boost = 1.0 + zeta_factor * 0.2 * max(0.0, cos_sim.item())\n",
        "                state['prev_grad'].copy_(grad)\n",
        "\n",
        "                m.mul_(beta1).add_(grad, alpha=1.0 - beta1)\n",
        "                v.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
        "\n",
        "                m_hat = m / (1.0 - beta1 ** step)\n",
        "                v_hat = v / (1.0 - beta2 ** step)\n",
        "\n",
        "                # Adam update\n",
        "                adam_update = m_hat / (torch.sqrt(v_hat) + eps)\n",
        "\n",
        "                # Zeta update\n",
        "                grad_norm = torch.sqrt(v_hat).add_(eps)\n",
        "                norm_factor = torch.clamp(grad_norm, max=0.5)\n",
        "                zeta_scaled_lr = lr * zeta_factor / (grad_norm ** (s - 1.0) * norm_factor)\n",
        "                zeta_update = zeta_scaled_lr * m_hat * momentum_boost\n",
        "\n",
        "                # Hybrid update\n",
        "                final_update = adam_mix * adam_update + (1.0 - adam_mix) * zeta_update\n",
        "\n",
        "                # Step-wise LR decay on plateau\n",
        "                lr_mult = 1.0 if loss_ema > 0.1 else 0.5\n",
        "                current_lr = lr * lr_mult * (0.5 * (1.0 + math.cos(math.pi * step / (total_steps * 1.2))))\n",
        "\n",
        "                p.data.add_(-current_lr * final_update)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "def get_cifar10_data(batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Training function with accuracy\n",
        "def train(model, optimizer, data_loader, criterion, epochs=5, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step(closure=lambda: criterion(model(inputs), labels))\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
        "    return model\n",
        "\n",
        "# Test function\n",
        "def test(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    batch_size = 64\n",
        "    lr_adam = 0.001\n",
        "    lr_sgd = 0.01\n",
        "    lr_zeta = 0.0015\n",
        "    epochs = 5\n",
        "    s_min = 1.001\n",
        "    s_max = 2.0\n",
        "    zeta_damp = 0.3\n",
        "    adam_mix = 0.5\n",
        "    total_steps = 5000  # Approx steps for 5 epochs (50000 samples / batch_size)\n",
        "\n",
        "    # Data\n",
        "    train_loader, test_loader = get_cifar10_data(batch_size=batch_size)\n",
        "\n",
        "    # Models\n",
        "    model_adam = SimpleNN()\n",
        "    model_sgd = SimpleNN()\n",
        "    model_zeta = SimpleNN()\n",
        "\n",
        "    # Loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_adam = optim.Adam(model_adam.parameters(), lr=lr_adam)\n",
        "    optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=lr_sgd, momentum=0.9)\n",
        "    optimizer_zeta = ZetaOptimizer(model_zeta.parameters(), lr=lr_zeta, s_min=s_min, s_max=s_max, beta1=0.85, beta2=0.999, eps=1e-8, clip_norm=1.0, zeta_damp=zeta_damp, adam_mix=adam_mix, total_steps=total_steps)\n",
        "\n",
        "    # Train with Adam\n",
        "    print(\"Training with Adam optimizer:\")\n",
        "    model_adam = train(model_adam, optimizer_adam, train_loader, criterion, epochs)\n",
        "    print(\"Evaluating Adam on test set:\")\n",
        "    test(model_adam, test_loader, criterion)\n",
        "\n",
        "    # Train with SGD\n",
        "    print(\"\\nTraining with SGD optimizer (with momentum):\")\n",
        "    model_sgd = train(model_sgd, optimizer_sgd, train_loader, criterion, epochs)\n",
        "    print(\"Evaluating SGD on test set:\")\n",
        "    test(model_sgd, test_loader, criterion)\n",
        "\n",
        "    # Train with ZetaOptimizer\n",
        "    print(\"\\nTraining with Final Enhanced Zeta-inspired optimizer:\")\n",
        "    model_zeta = train(model_zeta, optimizer_zeta, train_loader, criterion, epochs)\n",
        "    print(\"Evaluating ZetaOptimizer on test set:\")\n",
        "    test(model_zeta, test_loader, criterion)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4M5b49dBrFB",
        "outputId": "db97c20a-5565-457a-e7ac-d69820341fbe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Adam optimizer:\n",
            "Epoch 1/5, Train Loss: 1.7064, Train Accuracy: 41.01%\n",
            "Epoch 2/5, Train Loss: 1.4876, Train Accuracy: 48.25%\n",
            "Epoch 3/5, Train Loss: 1.4124, Train Accuracy: 50.54%\n",
            "Epoch 4/5, Train Loss: 1.3599, Train Accuracy: 52.79%\n",
            "Epoch 5/5, Train Loss: 1.3094, Train Accuracy: 54.31%\n",
            "Evaluating Adam on test set:\n",
            "Test Loss: 1.4453, Test Accuracy: 50.72%\n",
            "\n",
            "Training with SGD optimizer (with momentum):\n",
            "Epoch 1/5, Train Loss: 1.7495, Train Accuracy: 39.24%\n",
            "Epoch 2/5, Train Loss: 1.6289, Train Accuracy: 44.28%\n",
            "Epoch 3/5, Train Loss: 1.5783, Train Accuracy: 46.55%\n",
            "Epoch 4/5, Train Loss: 1.5474, Train Accuracy: 47.82%\n",
            "Epoch 5/5, Train Loss: 1.4977, Train Accuracy: 49.82%\n",
            "Evaluating SGD on test set:\n",
            "Test Loss: 1.6464, Test Accuracy: 45.32%\n",
            "\n",
            "Training with Final Enhanced Zeta-inspired optimizer:\n",
            "Epoch 1/5, Train Loss: 1.6772, Train Accuracy: 41.58%\n",
            "Epoch 2/5, Train Loss: 1.4535, Train Accuracy: 49.20%\n",
            "Epoch 3/5, Train Loss: 1.3517, Train Accuracy: 53.09%\n",
            "Epoch 4/5, Train Loss: 1.2591, Train Accuracy: 56.32%\n",
            "Epoch 5/5, Train Loss: 1.1754, Train Accuracy: 59.44%\n",
            "Evaluating ZetaOptimizer on test set:\n",
            "Test Loss: 1.3674, Test Accuracy: 52.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CIFAR-10**"
      ],
      "metadata": {
        "id": "qs2KMUQWHzdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "\n",
        "# Neural Network for CIFAR-100\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_channels=3, hidden_dim=256, output_dim=100):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_channels * 32 * 32, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Enhanced Zeta-inspired optimizer\n",
        "class ZetaOptimizer(optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, s_min=1.001, s_max=2.0, beta1=0.85, beta2=0.999, eps=1e-8, clip_norm=1.0, zeta_damp=0.3, adam_mix=0.7, total_steps=5000):\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(\"Learning rate must be positive\")\n",
        "        if s_min <= 1.0 or s_max <= 1.0:\n",
        "            raise ValueError(\"s_min and s_max must be > 1 for convergence\")\n",
        "        if not 0.0 <= beta1 < 1.0:\n",
        "            raise ValueError(\"Beta1 must be in [0, 1)\")\n",
        "        if not 0.0 <= beta2 < 1.0:\n",
        "            raise ValueError(\"Beta2 must be in [0, 1)\")\n",
        "        if eps <= 0.0:\n",
        "            raise ValueError(\"Epsilon must be positive\")\n",
        "        if clip_norm <= 0.0:\n",
        "            raise ValueError(\"Clip norm must be positive\")\n",
        "        if not 0.0 <= adam_mix <= 1.0:\n",
        "            raise ValueError(\"adam_mix must be in [0, 1]\")\n",
        "\n",
        "        defaults = dict(lr=lr, s_min=s_min, s_max=s_max, beta1=beta1, beta2=beta2, eps=eps, clip_norm=clip_norm, zeta_damp=zeta_damp, adam_mix=adam_mix, total_steps=total_steps)\n",
        "        super(ZetaOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        max_terms = 100\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            s_min = group['s_min']\n",
        "            s_max = group['s_max']\n",
        "            beta1 = group['beta1']\n",
        "            beta2 = group['beta2']\n",
        "            eps = group['eps']\n",
        "            clip_norm = group['clip_norm']\n",
        "            zeta_damp = group['zeta_damp']\n",
        "            adam_mix = group['adam_mix']\n",
        "            total_steps = group['total_steps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                grad = grad.clamp(min=-clip_norm, max=clip_norm)\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['m'] = torch.zeros_like(p.data)\n",
        "                    state['v'] = torch.zeros_like(p.data)\n",
        "                    state['grad_norm_ema'] = torch.zeros_like(p.data)\n",
        "                    state['prev_grad'] = torch.zeros_like(p.data)\n",
        "                    state['loss_ema'] = 1.0\n",
        "\n",
        "                state['step'] += 1\n",
        "                m, v = state['m'], state['v']\n",
        "                grad_norm_ema = state['grad_norm_ema']\n",
        "                prev_grad = state['prev_grad']\n",
        "                loss_ema = state['loss_ema']\n",
        "                step = state['step']\n",
        "\n",
        "                s = s_min + (s_max - s_min) * min(step / total_steps, 1.0)\n",
        "                zeta_s = sum(1.0 / (n ** s) for n in range(1, max_terms + 1))\n",
        "\n",
        "                grad_norm_raw = torch.sqrt((grad ** 2).sum(dim=-1, keepdim=True))\n",
        "                grad_norm_ema.mul_(0.9).add_(grad_norm_raw, alpha=0.1)\n",
        "\n",
        "                if closure is not None and loss is not None:\n",
        "                    state['loss_ema'] = 0.9 * loss_ema + 0.1 * loss.item()\n",
        "                adaptive_damp = zeta_damp * (1.0 + grad_norm_ema / (1.0 + grad_norm_ema)) * (1.0 / max(0.1, loss_ema))\n",
        "                zeta_factor = adaptive_damp / zeta_s * (1.0 / (1.0 + step * 0.005))\n",
        "\n",
        "                grad_flat = grad.view(-1)\n",
        "                prev_grad_flat = prev_grad.view(-1)\n",
        "                cos_sim = torch.dot(grad_flat, prev_grad_flat) / (grad_flat.norm() * prev_grad_flat.norm() + eps)\n",
        "                momentum_boost = 1.0 + zeta_factor * 0.2 * max(0.0, cos_sim.item())\n",
        "                state['prev_grad'].copy_(grad)\n",
        "\n",
        "                m.mul_(beta1).add_(grad, alpha=1.0 - beta1)\n",
        "                v.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
        "\n",
        "                m_hat = m / (1.0 - beta1 ** step)\n",
        "                v_hat = v / (1.0 - beta2 ** step)\n",
        "\n",
        "                adam_update = m_hat / (torch.sqrt(v_hat) + eps)\n",
        "\n",
        "                grad_norm = torch.sqrt(v_hat).add_(eps)\n",
        "                norm_factor = torch.clamp(grad_norm, max=0.5)\n",
        "                zeta_scaled_lr = lr * zeta_factor / (grad_norm ** (s - 1.0) * norm_factor)\n",
        "                zeta_update = zeta_scaled_lr * m_hat * momentum_boost\n",
        "\n",
        "                final_update = adam_mix * adam_update + (1.0 - adam_mix) * zeta_update\n",
        "\n",
        "                lr_mult = 1.0 if loss_ema > 0.1 else 0.5\n",
        "                current_lr = lr * lr_mult * (0.5 * (1.0 + math.cos(math.pi * step / (total_steps * 1.2))))\n",
        "\n",
        "                p.data.add_(-current_lr * final_update)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "def get_cifar100_data(batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "    train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Training function with accuracy\n",
        "def train(model, optimizer, data_loader, criterion, epochs=5, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step(closure=lambda: criterion(model(inputs), labels))\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
        "    return model\n",
        "\n",
        "# Test function\n",
        "def test(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    batch_size = 64\n",
        "    lr_adam = 0.001\n",
        "    lr_sgd = 0.01\n",
        "    lr_zeta = 0.001\n",
        "    epochs = 20\n",
        "    s_min = 1.001\n",
        "    s_max = 2.0\n",
        "    zeta_damp = 0.3\n",
        "    adam_mix = 0.7\n",
        "    total_steps = 5000  # Approx steps for 5 epochs (50000 samples / batch_size)\n",
        "\n",
        "    # Data\n",
        "    train_loader, test_loader = get_cifar100_data(batch_size=batch_size)\n",
        "\n",
        "    # Models\n",
        "    model_adam = SimpleNN()\n",
        "    model_sgd = SimpleNN()\n",
        "    model_zeta = SimpleNN()\n",
        "\n",
        "    # Loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_adam = optim.Adam(model_adam.parameters(), lr=lr_adam)\n",
        "    optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=lr_sgd, momentum=0.9)\n",
        "    optimizer_zeta = ZetaOptimizer(model_zeta.parameters(), lr=lr_zeta, s_min=s_min, s_max=s_max, beta1=0.85, beta2=0.999, eps=1e-8, clip_norm=1.0, zeta_damp=zeta_damp, adam_mix=adam_mix, total_steps=total_steps)\n",
        "\n",
        "    # Train with Adam\n",
        "    print(\"Training with Adam optimizer:\")\n",
        "    model_adam = train(model_adam, optimizer_adam, train_loader, criterion, epochs)\n",
        "    print(\"Evaluating Adam on test set:\")\n",
        "    test(model_adam, test_loader, criterion)\n",
        "\n",
        "    # Train with SGD\n",
        "    print(\"\\nTraining with SGD optimizer (with momentum):\")\n",
        "    model_sgd = train(model_sgd, optimizer_sgd, train_loader, criterion, epochs)\n",
        "    print(\"Evaluating SGD on test set:\")\n",
        "    test(model_sgd, test_loader, criterion)\n",
        "\n",
        "    # Train with ZetaOptimizer\n",
        "    print(\"\\nTraining with Final Enhanced Zeta-inspired optimizer:\")\n",
        "    model_zeta = train(model_zeta, optimizer_zeta, train_loader, criterion, epochs)\n",
        "    print(\"Evaluating ZetaOptimizer on test set:\")\n",
        "    test(model_zeta, test_loader, criterion)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEX6QGluFOz6",
        "outputId": "92929472-6659-4c62-fb34-f29d7f882417"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Adam optimizer:\n",
            "Epoch 1/20, Train Loss: 3.7533, Train Accuracy: 13.49%\n",
            "Epoch 2/20, Train Loss: 3.3757, Train Accuracy: 19.42%\n",
            "Epoch 3/20, Train Loss: 3.1953, Train Accuracy: 22.67%\n",
            "Epoch 4/20, Train Loss: 3.0651, Train Accuracy: 24.91%\n",
            "Epoch 5/20, Train Loss: 2.9407, Train Accuracy: 27.60%\n",
            "Epoch 6/20, Train Loss: 2.8321, Train Accuracy: 29.29%\n",
            "Epoch 7/20, Train Loss: 2.7379, Train Accuracy: 31.10%\n",
            "Epoch 8/20, Train Loss: 2.6352, Train Accuracy: 33.03%\n",
            "Epoch 9/20, Train Loss: 2.5409, Train Accuracy: 35.02%\n",
            "Epoch 10/20, Train Loss: 2.4469, Train Accuracy: 36.48%\n",
            "Epoch 11/20, Train Loss: 2.3633, Train Accuracy: 38.74%\n",
            "Epoch 12/20, Train Loss: 2.2834, Train Accuracy: 40.04%\n",
            "Epoch 13/20, Train Loss: 2.1946, Train Accuracy: 42.20%\n",
            "Epoch 14/20, Train Loss: 2.1243, Train Accuracy: 43.64%\n",
            "Epoch 15/20, Train Loss: 2.0560, Train Accuracy: 45.03%\n",
            "Epoch 16/20, Train Loss: 1.9885, Train Accuracy: 46.66%\n",
            "Epoch 17/20, Train Loss: 1.9149, Train Accuracy: 48.38%\n",
            "Epoch 18/20, Train Loss: 1.8697, Train Accuracy: 49.19%\n",
            "Epoch 19/20, Train Loss: 1.8019, Train Accuracy: 50.95%\n",
            "Epoch 20/20, Train Loss: 1.7525, Train Accuracy: 51.94%\n",
            "Evaluating Adam on test set:\n",
            "Test Loss: 4.7089, Test Accuracy: 21.70%\n",
            "\n",
            "Training with SGD optimizer (with momentum):\n",
            "Epoch 1/20, Train Loss: 3.8002, Train Accuracy: 12.44%\n",
            "Epoch 2/20, Train Loss: 3.3684, Train Accuracy: 19.60%\n",
            "Epoch 3/20, Train Loss: 3.1636, Train Accuracy: 23.28%\n",
            "Epoch 4/20, Train Loss: 3.0140, Train Accuracy: 26.18%\n",
            "Epoch 5/20, Train Loss: 2.8884, Train Accuracy: 28.32%\n",
            "Epoch 6/20, Train Loss: 2.7797, Train Accuracy: 30.44%\n",
            "Epoch 7/20, Train Loss: 2.6760, Train Accuracy: 32.74%\n",
            "Epoch 8/20, Train Loss: 2.5842, Train Accuracy: 34.30%\n",
            "Epoch 9/20, Train Loss: 2.4895, Train Accuracy: 36.33%\n",
            "Epoch 10/20, Train Loss: 2.4049, Train Accuracy: 37.89%\n",
            "Epoch 11/20, Train Loss: 2.3352, Train Accuracy: 39.35%\n",
            "Epoch 12/20, Train Loss: 2.2466, Train Accuracy: 41.21%\n",
            "Epoch 13/20, Train Loss: 2.1843, Train Accuracy: 42.62%\n",
            "Epoch 14/20, Train Loss: 2.1178, Train Accuracy: 44.13%\n",
            "Epoch 15/20, Train Loss: 2.0530, Train Accuracy: 45.31%\n",
            "Epoch 16/20, Train Loss: 1.9903, Train Accuracy: 46.78%\n",
            "Epoch 17/20, Train Loss: 1.9379, Train Accuracy: 48.20%\n",
            "Epoch 18/20, Train Loss: 1.8852, Train Accuracy: 49.31%\n",
            "Epoch 19/20, Train Loss: 1.8533, Train Accuracy: 49.81%\n",
            "Epoch 20/20, Train Loss: 1.7791, Train Accuracy: 51.60%\n",
            "Evaluating SGD on test set:\n",
            "Test Loss: 4.3668, Test Accuracy: 22.51%\n",
            "\n",
            "Training with Final Enhanced Zeta-inspired optimizer:\n",
            "Epoch 1/20, Train Loss: 3.7273, Train Accuracy: 14.12%\n",
            "Epoch 2/20, Train Loss: 3.2976, Train Accuracy: 21.10%\n",
            "Epoch 3/20, Train Loss: 3.0498, Train Accuracy: 25.56%\n",
            "Epoch 4/20, Train Loss: 2.8216, Train Accuracy: 30.35%\n",
            "Epoch 5/20, Train Loss: 2.6025, Train Accuracy: 34.53%\n",
            "Epoch 6/20, Train Loss: 2.4213, Train Accuracy: 38.84%\n",
            "Epoch 7/20, Train Loss: 2.2988, Train Accuracy: 41.79%\n",
            "Epoch 8/20, Train Loss: 2.2443, Train Accuracy: 43.01%\n",
            "Epoch 9/20, Train Loss: 2.2495, Train Accuracy: 42.96%\n",
            "Epoch 10/20, Train Loss: 2.2837, Train Accuracy: 42.07%\n",
            "Epoch 11/20, Train Loss: 2.3218, Train Accuracy: 40.97%\n",
            "Epoch 12/20, Train Loss: 2.3522, Train Accuracy: 39.76%\n",
            "Epoch 13/20, Train Loss: 2.3656, Train Accuracy: 39.04%\n",
            "Epoch 14/20, Train Loss: 2.3630, Train Accuracy: 39.02%\n",
            "Epoch 15/20, Train Loss: 2.3193, Train Accuracy: 39.91%\n",
            "Epoch 16/20, Train Loss: 2.2354, Train Accuracy: 41.79%\n",
            "Epoch 17/20, Train Loss: 2.1023, Train Accuracy: 44.65%\n",
            "Epoch 18/20, Train Loss: 1.9141, Train Accuracy: 49.08%\n",
            "Epoch 19/20, Train Loss: 1.6955, Train Accuracy: 54.51%\n",
            "Epoch 20/20, Train Loss: 1.4711, Train Accuracy: 60.31%\n",
            "Evaluating ZetaOptimizer on test set:\n",
            "Test Loss: 3.8650, Test Accuracy: 25.71%\n"
          ]
        }
      ]
    }
  ]
}